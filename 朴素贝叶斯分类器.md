朴素贝叶斯分类器是基于贝叶斯定理的分类模型。
##1. 朴素贝叶斯分类器的优缺点
这里直接给出结论，后续文章分析贝叶斯的优缺点。
>优点：
朴素贝叶斯模型发源于古典数学理论，有着坚实的数学基础，以及稳定的分类效率。
对小规模的数据表现很好，能个处理多分类任务，适合增量式训练；
对缺失数据不太敏感，算法也比较简单，常用于文本分类。
>缺点：
需要计算先验概率；
分类决策存在错误率；
对输入数据的表达形式很敏感。
##2. 贝叶斯定理
朴素贝叶斯算法是根据贝叶斯定理来实现的。
简洁形式：
$P\left ( A |B \right )=\frac{P(B|A) \times P(A)}{P(B)}$
现实中很少有一件事物只有一个影响因素，假设事件B$\begin{pmatrix}
b_{1} & ...  & b_{n}
\end{pmatrix}$，有n个影响因素。
则P(A|B)可以表示为：
$P(A|b_{1} ... b_{n}) = \frac{P(A)P(b_{1}...b_{n}|A)}{P(b_{1}...b_{n})}$
先验概率P(A)、联合概率$P(b_{1}...b_{n})$可以看作常量。
$P(b_{1}...b_{n}|A)$ 通过条件概率公式+链式法则得：
$P(b_{1}...b_{n}|A)=P(b_{1}|A)P(b_{2}|A,b_{1})...P(b_{i}|A,b_{1},...,b_{i-1})...P(b_{n}|A,b_{1}...b_{n-1}))$
朴素贝叶斯的一个重要的假设：***所有的特征是相互独立的***。这个假设极大简化了计算的复杂性，因此上式可以化简为：
$P(b_{1}...b_{n}|A)=P(b_{1}|A)P(b_{2}|A)...P(b_{i}|A)...P(b_{n}|A))=\prod_{1}^{n}P(b_{i}|A)$
综上所述，贝叶斯公式形式如下：
> $P(A|b_{1} ... b_{n}) = \frac{1}{Z}P(A)\prod_{1}^{n}P(b_{i}|A) \\
Z=P(b_{1}...b_{n})$


##3.简单的朴素贝叶斯分类器
上述的$b_{1}...b_{n}$对应样本数据的特征$F_{1}...F{n}$，A对应的类别C，因此，贝叶斯公式可以转换为
> $P(C|F_{1} ... F_{n}) = \frac{1}{Z}P(C)\prod_{1}^{n}P(F_{i}|C) \\
Z=P(F_{1}...F_{n})$

这就是朴素贝叶斯分类器的模型函数。

#####如何使用朴素贝叶斯分类器分类预测？
> step1. 假设样本特征$F_{1}...F{n}$，类别$C_{1}...C{m}$。现有样本数据S，分别计算：$P(C_{i})$、$\prod_{1}^{n}P(F_{i}=f_{i}|C_{i})$。因为$Z$对于每个 $P(C_{i}|F_{1} ... F_{n})$在同一个数据集下值是相同的，因此忽略此项。
>
>step2. $max(P(C_{i}|F_{1} ... F_{n}))$对应的$C_{i}$即为此样本所属类别。

######上述计算方式是否有疑问？
朴素贝叶斯分类器这个模型的训练过程都不需要先从模型函数推导目标函数，再优化目标函数求 Cost 最小的解吗？朴素贝叶斯公式就是朴素贝叶斯分类器的训练算法啦？
上述例子之所以这样简单，是因为我们简单地将频率当成了概率。
但在现实应用中，这种方法往往不可行，因为这种方法实际上默认了“未被观测到”的就是“出现概率为0”的。这样做显然是不合理的。

比如：如果由于样本量太小，某个样本的特征取值$F_{i}=f_{i}$没有在类别$C_{i}$中出现，那么$\prod_{1}^{n}P(F_{i}=f_{i}|C_{i})=0$，此样本数据属于$C_{i}$的可能性为0，这显然不合理。

虽然我们可以靠做一些简单的变换——比如加一平滑法（就是把概率计算为：对应类别样本中该特征值出现次数 + 1 /对应类别样本总数）——来规避除以0的情况，但是这样做的“准确性”仍然非常不可靠。

我们有没有更加精准的方法呢？当然有！
#####解决除以0的问题：最大似然估计。
出现上述问题的原因是**利用频率估算概率$P(F_{i}=f_{i}|C_{i})$**，解决此问题，可以换一种估算概率的方法。
根据样本数据估算概率的常用方法：**最大似然估计**。
**最大似然估计**是基于世界事物概率分布是确定，观察到的数据围绕确定的值波动的一个区间范围。
最大似然估计的策略：
>step1. 假定样本的特征具有某个确定的概率分布形式（比如：正态分布，泊松分布等）。
step2. 根据样本数据对特征的概率分布参数（比如：正态分布的均值$\mu$和方差$\theta$）进行计算。计算的依据是：使得样本数据出现的概率最大化即可能性最大的参数值即为所求的参数值。

最大似然估计认为只有在最接近固定参数值的情况下，样本数据才能出现；反过来讲就是样本数据出现的概率最大的参数估计才是最接近本源参数的估计。

参数$\theta_{c,i}$的似然函数记作 $L(\theta_{c,i})$，表示样本$D_{c}$有 
$m_{c}$个样本$ X_{1}，X_{2}，…X_{mc} $在第 i 个特征上的联合概率分布：
$L(\theta_{c,i})=\prod_{1}^{mc}P(x_{i}^{(j)}|\theta_{c,i})$
求使$L(\theta_{c,i})$取最大值的$\theta_{c,i}$的值。
**计算方法：**
> step1. 对等式两边取对数log，因为log运算是连续递增函数上凸函数与原式有相同的增减性，所以，转换求$max(log(L(\theta_{c,i})))$。
$LL(\theta_{c,i})=\sum_{1}^{mc}P(x_{i}^{(j)}|\theta_{c,i})$
step2. 求最大值的常用方法就是求极值，极值出导数为0。而log函数是连续上凸函数，可导极值点存在。

确定了参数$\theta_{c,i}$的值，代入概率分布函数中，就得到了确定的特征i的概率分布函数。到此为止，我们就可以确定$P(F_{i}=f_{i}|C_{i})$的在特征$F_{i}=f_{i}$处的概率值。




